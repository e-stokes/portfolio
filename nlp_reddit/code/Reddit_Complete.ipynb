{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments:\n",
    "All code that is commented out, is for the purpose of not running the web scrape again on the reddit API\n",
    "\n",
    "Also csv save points are commented out as well to prevent overwriting save points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import requests\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime as dt\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn import feature_selection\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for scraping web (Provided by Josh Robin)\n",
    "# This is a better web scrape function allows to skip = days, and times = amount to run\n",
    "\n",
    "# this function was slightly modified from Brian Collins' lecture\n",
    "def query_pushshift(subreddit, kind='submission', skip=30, times=30, \n",
    "                    subfield = ['title', 'selftext', 'subreddit', 'created_utc', 'author', 'num_comments', 'score', 'is_self'],\n",
    "                    comfields = ['body', 'score', 'created_utc']):\n",
    "\n",
    "    stem = \"https://api.pushshift.io/reddit/search/{}/?subreddit={}&size=500\".format(kind, subreddit)\n",
    "    mylist = []\n",
    "    \n",
    "    for x in range(1, times):\n",
    "        \n",
    "        URL = \"{}&after={}d\".format(stem, skip * x)\n",
    "        print(URL)\n",
    "        response = requests.get(URL)\n",
    "        assert response.status_code == 200\n",
    "        mine = response.json()['data']\n",
    "        df = pd.DataFrame.from_dict(mine)\n",
    "        mylist.append(df)\n",
    "        time.sleep(2)\n",
    "        \n",
    "    full = pd.concat(mylist, sort=False)\n",
    "    \n",
    "    if kind == \"submission\":\n",
    "        \n",
    "        full = full[subfield]\n",
    "        \n",
    "        full = full.drop_duplicates()\n",
    "        \n",
    "        full = full.loc[full['is_self'] == True]\n",
    "        \n",
    "    def get_date(created):\n",
    "        return dt.date.fromtimestamp(created)\n",
    "    \n",
    "    _timestamp = full[\"created_utc\"].apply(get_date)\n",
    "    \n",
    "    full['timestamp'] = _timestamp\n",
    "\n",
    "    print(full.shape)\n",
    "    \n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scraping Reddit website subreddit Fantasy Hockey\n",
    "\n",
    "# df_fan = query_pushshift('fantasyhockey', times = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chcecking unique posts within self text (post body)\n",
    "len(df_fan['selftext'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scraping Reddit website subreddit Official NHL\n",
    "\n",
    "# df_nhl = query_pushshift('nhl', times = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking unique values within NHL subreddit\n",
    "len(df_nhl['selftext'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving raw scrap csv data\n",
    "# df_fan.to_csv('./fan_raw.csv')\n",
    "# df_nhl.to_csv('./nhl_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing all columns except 'selftext', 'title', 'subreddit'\n",
    "cols = ['selftext', 'title', 'subreddit']\n",
    "df_fan = df_fan[cols]\n",
    "df_nhl = df_nhl[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking dataframe changes for Fantasy Hockey\n",
    "df_fan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking dataframe changes for Official NHL\n",
    "df_nhl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using RegEx ro remove misc characters on Fantasy selftext\n",
    "df_fan['selftext'] = df_fan['selftext'].str.replace('\\s[\\/]?r\\/[^s]+', ' ')\n",
    "df_fan['selftext'] = df_fan['selftext'].str.replace('http[s]?:\\/\\/[^\\s]*', ' ')\n",
    "df_fan['selftext'] = df_fan['selftext'].str.replace(\"[^a-zA-Z]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using RegEx ro remove misc characters on Official NHL selftext\n",
    "df_nhl['selftext'] = df_nhl['selftext'].str.replace('\\s[\\/]?r\\/[^s]+', ' ')\n",
    "df_nhl['selftext'] = df_nhl['selftext'].str.replace('http[s]?:\\/\\/[^\\s]*', ' ')\n",
    "df_nhl['selftext'] = df_nhl['selftext'].str.replace(\"[^a-zA-Z]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using RegEx ro remove misc characters on Fantasy title\n",
    "df_fan['title'] = df_fan['title'].str.replace('\\s[\\/]?r\\/[^s]+', ' ')\n",
    "df_fan['title'] = df_fan['title'].str.replace('http[s]?:\\/\\/[^\\s]*', ' ')\n",
    "df_fan['title'] = df_fan['title'].str.replace(\"[^a-zA-Z]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking changes in Fantasy dataframe\n",
    "df_fan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using RegEx ro remove misc characters on Official NHL title\n",
    "df_nhl['title'] = df_nhl['title'].str.replace('\\s[\\/]?r\\/[^s]+', ' ')\n",
    "df_nhl['title'] = df_nhl['title'].str.replace('http[s]?:\\/\\/[^\\s]*', ' ')\n",
    "df_nhl['title'] = df_nhl['title'].str.replace(\"[^a-zA-Z]\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking changes in Official NHL dataframe\n",
    "df_nhl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values in Fantasy dataframe\n",
    "df_fan.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking dataframe shape before dropping nulls in Fantasy hockey dataframe\n",
    "df_fan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping null values in Fantasy dataframe\n",
    "df_fan.dropna(inplace = True)\n",
    "# Checking shape after drop\n",
    "df_fan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking null values in NHL dataframe\n",
    "df_nhl.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking shape before dropping nulls in NHL dataframe\n",
    "df_nhl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping null values in NHL dataframe\n",
    "df_nhl.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking shape for NHL after drop\n",
    "df_nhl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving cleaned versions to csv\n",
    "\n",
    "# df_fan.to_csv('./fan_clean.csv')\n",
    "# df_nhl.to_csv('./nhl_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating both Dataframes together\n",
    "df = pd.concat([df_fan, df_nhl])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking shape after concat\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking empty values and changing them to null values in dataframe\n",
    "df.replace('', np.nan, inplace = True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping null values in dataframe\n",
    "df.dropna(inplace = True)\n",
    "# checking shape after drop\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy columns for subreddit\n",
    "df['subreddit'] = pd.get_dummies(df['subreddit'], drop_first=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding selftext and title together in df\n",
    "df[\"text\"] = df[\"selftext\"].map(str) + df[\"title\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data as csv\n",
    "# df.to_csv('./reddit_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Please be nice to each other  Upvote useful co...</td>\n",
       "      <td>Daily Anything Goes   June</td>\n",
       "      <td>0</td>\n",
       "      <td>Please be nice to each other  Upvote useful co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Give and receive fantasy team advice in this t...</td>\n",
       "      <td>Roster Management   June</td>\n",
       "      <td>0</td>\n",
       "      <td>Give and receive fantasy team advice in this t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Please be nice to each other  Upvote useful co...</td>\n",
       "      <td>Nightly Anything Goes   June</td>\n",
       "      <td>0</td>\n",
       "      <td>Please be nice to each other  Upvote useful co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Please be nice to each other  Upvote useful co...</td>\n",
       "      <td>Daily Anything Goes   June</td>\n",
       "      <td>0</td>\n",
       "      <td>Please be nice to each other  Upvote useful co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Give and receive fantasy team advice in this t...</td>\n",
       "      <td>Roster Management   June</td>\n",
       "      <td>0</td>\n",
       "      <td>Give and receive fantasy team advice in this t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           selftext  \\\n",
       "0           0  Please be nice to each other  Upvote useful co...   \n",
       "1           1  Give and receive fantasy team advice in this t...   \n",
       "2           2  Please be nice to each other  Upvote useful co...   \n",
       "3           3  Please be nice to each other  Upvote useful co...   \n",
       "4           4  Give and receive fantasy team advice in this t...   \n",
       "\n",
       "                                   title  subreddit  \\\n",
       "0    Daily Anything Goes   June                   0   \n",
       "1      Roster Management   June                   0   \n",
       "2  Nightly Anything Goes   June                   0   \n",
       "3    Daily Anything Goes   June                   0   \n",
       "4      Roster Management   June                   0   \n",
       "\n",
       "                                                text  \n",
       "0  Please be nice to each other  Upvote useful co...  \n",
       "1  Give and receive fantasy team advice in this t...  \n",
       "2  Please be nice to each other  Upvote useful co...  \n",
       "3  Please be nice to each other  Upvote useful co...  \n",
       "4  Give and receive fantasy team advice in this t...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing above csv since restarting notebook\n",
    "# df = pd.read_csv('./reddit_complete.csv')\n",
    "# checking df import\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please be nice to each other  Upvote useful co...</td>\n",
       "      <td>Daily Anything Goes   June</td>\n",
       "      <td>0</td>\n",
       "      <td>Please be nice to each other  Upvote useful co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Give and receive fantasy team advice in this t...</td>\n",
       "      <td>Roster Management   June</td>\n",
       "      <td>0</td>\n",
       "      <td>Give and receive fantasy team advice in this t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please be nice to each other  Upvote useful co...</td>\n",
       "      <td>Nightly Anything Goes   June</td>\n",
       "      <td>0</td>\n",
       "      <td>Please be nice to each other  Upvote useful co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Please be nice to each other  Upvote useful co...</td>\n",
       "      <td>Daily Anything Goes   June</td>\n",
       "      <td>0</td>\n",
       "      <td>Please be nice to each other  Upvote useful co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Give and receive fantasy team advice in this t...</td>\n",
       "      <td>Roster Management   June</td>\n",
       "      <td>0</td>\n",
       "      <td>Give and receive fantasy team advice in this t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            selftext  \\\n",
       "0  Please be nice to each other  Upvote useful co...   \n",
       "1  Give and receive fantasy team advice in this t...   \n",
       "2  Please be nice to each other  Upvote useful co...   \n",
       "3  Please be nice to each other  Upvote useful co...   \n",
       "4  Give and receive fantasy team advice in this t...   \n",
       "\n",
       "                                   title  subreddit  \\\n",
       "0    Daily Anything Goes   June                   0   \n",
       "1      Roster Management   June                   0   \n",
       "2  Nightly Anything Goes   June                   0   \n",
       "3    Daily Anything Goes   June                   0   \n",
       "4      Roster Management   June                   0   \n",
       "\n",
       "                                                text  \n",
       "0  Please be nice to each other  Upvote useful co...  \n",
       "1  Give and receive fantasy team advice in this t...  \n",
       "2  Please be nice to each other  Upvote useful co...  \n",
       "3  Please be nice to each other  Upvote useful co...  \n",
       "4  Give and receive fantasy team advice in this t...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping unnamed column in df\n",
    "df.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "# Checking df changes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for wordcloud found \n",
    "\n",
    "# https://github.com/amueller/word_cloud/issues/52\n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None,\n",
    "                    **kwargs):\n",
    "    return \"hsl(9, 100%%, %d%%)\" % random.randint(40, 100)\n",
    "\n",
    "# found on https://stackoverflow.com/questions/16645799/how-to-create-a-word-cloud-from-a-corpus-in-python\n",
    "def create_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(color_func= grey_color_func,\n",
    "        background_color='white', # Changes background color\n",
    "        stopwords=stopwords, # Adds stop words\n",
    "        max_words=200, # Max words\n",
    "        max_font_size=40, # font size\n",
    "        scale=3 \n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X, y Variables\n",
    "X, y = df['selftext'], df['subreddit']\n",
    "\n",
    "# Setting up train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline & Gridsearch setup\n",
    "# TFIDF pipeline setup\n",
    "tvc_pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('mb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# CountVectorizer pipeline setup\n",
    "cv_pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('mb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Randomforest pipeline setup\n",
    "rf_pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Fit\n",
    "cv_pipe.fit(X_train, y_train)\n",
    "tvc_pipe.fit(X_train, y_train)\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Setting params for CountVectorizer gridsearch\n",
    "cvec_params = {\n",
    "    'cvec__max_features': [100, 2000],\n",
    "    'cvec__ngram_range': [(1, 1),(1, 2), (2, 2)],\n",
    "    'cvec__stop_words': [None, 'english']\n",
    "}\n",
    "\n",
    "# Setting params for TFIDF Vectorizer gridsearch\n",
    "tf_params = {\n",
    "    'tvec__max_features':[100, 2000],\n",
    "    'tvec__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "   \n",
    "}\n",
    "\n",
    "# Setting up randomforest params\n",
    "rf_params = {\n",
    "    'tvec__max_features':[2000],\n",
    "    'tvec__ngram_range': [(1, 2)],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'rf__max_depth': [1000],\n",
    "    'rf__min_samples_split': [100],\n",
    "    'rf__max_leaf_nodes': [None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running many features, these were the best parameters\n",
    "\n",
    "# {'rf__max_depth': 1000,\n",
    "#  'rf__max_leaf_nodes': None,\n",
    "#  'rf__min_samples_split': 100,\n",
    "#  'tvec__max_features': 2000,\n",
    "#  'tvec__ngram_range': (1, 2),\n",
    "#  'tvec__stop_words': 'english'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   16.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   21.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tvec',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words...\n",
       "                                                               oob_score=False,\n",
       "                                                               random_state=None,\n",
       "                                                               verbose=0,\n",
       "                                                               warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'rf__max_depth': [1000], 'rf__max_leaf_nodes': [None],\n",
       "                         'rf__min_samples_split': [100],\n",
       "                         'tvec__max_features': [2000],\n",
       "                         'tvec__ngram_range': [(1, 2)],\n",
       "                         'tvec__stop_words': ['english']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up GridSearch for Randomforest\n",
    "rf_gs = GridSearchCV(rf_pipe, param_grid=rf_params, cv = 5, verbose = 1, n_jobs = -1)\n",
    "\n",
    "# Setting up GridSearch for CountVectorizer\n",
    "cv_gs = GridSearchCV(cv_pipe, param_grid=cvec_params, cv = 5, verbose = 1, n_jobs = -1)\n",
    "\n",
    "# Fitting CountVectorizer GS\n",
    "cv_gs.fit(X_train, y_train)\n",
    "\n",
    "# Setting up GridSearch for TFIDFVectorizer\n",
    "tvc_gs = GridSearchCV(tvc_pipe, param_grid=tf_params, cv = 5, verbose =1, n_jobs = -1)\n",
    "\n",
    "# Fitting CV GS\n",
    "tvc_gs.fit(X_train, y_train)\n",
    "\n",
    "# Fitting Randomforest CV GS\n",
    "rf_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model file in Pickle (rf_gs)\n",
    "# pkl_file_1 = \"rf_selftext.pkl\"  \n",
    "# with open(pkl_file_1, 'wb') as file:  \n",
    "#     pickle.dump(rf_gs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model file in Pickle (cv_gs)\n",
    "# pkl_file_2 = \"cv_selftext.pkl\"  \n",
    "# with open(pkl_file_2, 'wb') as file:  \n",
    "#     pickle.dump(cv_gs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model file in Pickle (tvc_gs)\n",
    "# pkl_file_3 = \"tvc_selftext.pkl\"  \n",
    "# with open(pkl_file_3, 'wb') as file:  \n",
    "#     pickle.dump(tvc_gs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in model rf_gs\n",
    "with open(pkl_file_1, 'rb') as file:  \n",
    "    pickle_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in model cv_gs\n",
    "with open(pkl_file_2, 'rb') as file:  \n",
    "    pickle_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in model tvc_gs\n",
    "with open(pkl_file_3, 'rb') as file:  \n",
    "    pickle_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8442436264785835"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring Training data on CountVectorizer\n",
    "cv_gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8336271485235787"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring Test data on CountVectorizer\n",
    "cv_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8742193813827052"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring Training data on TFIDFVectorizer\n",
    "tvc_gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8627148523578669"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring Test data on TFIDFVectorizer\n",
    "tvc_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9380648005289839"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring Training data on RandomForest\n",
    "rf_gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.881004847950639"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking Test score on RandomForest\n",
    "rf_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Checking best parameters\n",
    "pickle_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a new df for  feature importance Random Forest\n",
    "# Code from Stack Overflow\n",
    "rf_df = pd.DataFrame(rf_pipe.steps[1][1].feature_importances_, rf_pipe.steps[0][1].get_feature_names(), columns=['importance'])\n",
    "rf_df.sort_values('importance', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting Random Forest Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting Top 20 Words in Random Forest\n",
    "plt.figure(figsize=(20,10))\n",
    "temp = rf_df.sort_values('importance', ascending = False).head(20)\n",
    "plt.barh(temp.index, temp['importance'])\n",
    "plt.title('Top 20 Words', fontsize=40)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xlabel('Amount of Information Gained', fontsize=30)\n",
    "plt.ylabel('Word', fontsize=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(rf_df.sort_values('importance', ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling on 'title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X, y Variables\n",
    "X, y = df['title'], df['subreddit']\n",
    "\n",
    "# Setting up train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline & Gridsearch setup\n",
    "# TFIDF pipeline setup\n",
    "tvc_pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('mb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# CountVectorizer pipeline setup\n",
    "cv_pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('mb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Randomforest pipeline setup\n",
    "rf_pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Fit\n",
    "cv_pipe.fit(X_train, y_train)\n",
    "tvc_pipe.fit(X_train, y_train)\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Setting params for CountVectorizer gridsearch\n",
    "cvec_params = {\n",
    "    'cvec__max_features': [100, 2000],\n",
    "    'cvec__ngram_range': [(1, 1),(1, 2), (2, 2)],\n",
    "    'cvec__stop_words': [None, 'english']\n",
    "}\n",
    "\n",
    "# Setting params for TFIDF Vectorizer gridsearch\n",
    "tf_params = {\n",
    "    'tvec__max_features':[100, 2000],\n",
    "    'tvec__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "   \n",
    "}\n",
    "\n",
    "# Setting up randomforest params\n",
    "rf_params = {\n",
    "    'tvec__max_features':[2000],\n",
    "    'tvec__ngram_range': [(1, 2)],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'rf__max_depth': [1000],\n",
    "    'rf__min_samples_split': [100],\n",
    "    'rf__max_leaf_nodes': [None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up GridSearch for Randomforest\n",
    "rf_gs = GridSearchCV(rf_pipe, param_grid=rf_params, cv = 5, verbose = 1, n_jobs = -1)\n",
    "\n",
    "# Setting up GridSearch for CountVectorizer\n",
    "cv_gs = GridSearchCV(cv_pipe, param_grid=cvec_params, cv = 5, verbose = 1, n_jobs = -1)\n",
    "\n",
    "# Fitting CountVectorizer GS\n",
    "cv_gs.fit(X_train, y_train)\n",
    "\n",
    "# Setting up GridSearch for TFIDFVectorizer\n",
    "tvc_gs = GridSearchCV(tvc_pipe, param_grid=tf_params, cv = 5, verbose =1, n_jobs = -1)\n",
    "\n",
    "# Fitting CV GS\n",
    "tvc_gs.fit(X_train, y_train)\n",
    "\n",
    "# Fitting Randomforest CV GS\n",
    "rf_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring Random Forest train\n",
    "rf_gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring Random Forest test\n",
    "rf_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_title = pd.DataFrame(rf_pipe.steps[1][1].feature_importances_, rf_pipe.steps[0][1].get_feature_names(), columns=['importance'])\n",
    "rf_title.sort_values('importance', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling on 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X, y Variables\n",
    "X, y = df['text'], df['subreddit']\n",
    "\n",
    "# Setting up train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline & Gridsearch setup\n",
    "# TFIDF pipeline setup\n",
    "tvc_pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('mb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# CountVectorizer pipeline setup\n",
    "cv_pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('mb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Randomforest pipeline setup\n",
    "rf_pipe = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Fit\n",
    "cv_pipe.fit(X_train, y_train)\n",
    "tvc_pipe.fit(X_train, y_train)\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Setting params for CountVectorizer gridsearch\n",
    "cvec_params = {\n",
    "    'cvec__max_features': [100, 2000],\n",
    "    'cvec__ngram_range': [(1, 1),(1, 2), (2, 2)],\n",
    "    'cvec__stop_words': [None, 'english']\n",
    "}\n",
    "\n",
    "# Setting params for TFIDF Vectorizer gridsearch\n",
    "tf_params = {\n",
    "    'tvec__max_features':[100, 2000],\n",
    "    'tvec__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tvec__stop_words': [None, 'english'],\n",
    "   \n",
    "}\n",
    "\n",
    "# Setting up randomforest params\n",
    "rf_params = {\n",
    "    'tvec__max_features':[2000],\n",
    "    'tvec__ngram_range': [(1, 2)],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'rf__max_depth': [1000],\n",
    "    'rf__min_samples_split': [100],\n",
    "    'rf__max_leaf_nodes': [None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    9.6s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   18.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   19.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tvec',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words...\n",
       "                                                               oob_score=False,\n",
       "                                                               random_state=None,\n",
       "                                                               verbose=0,\n",
       "                                                               warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'rf__max_depth': [1000], 'rf__max_leaf_nodes': [None],\n",
       "                         'rf__min_samples_split': [100],\n",
       "                         'tvec__max_features': [2000],\n",
       "                         'tvec__ngram_range': [(1, 2)],\n",
       "                         'tvec__stop_words': ['english']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up GridSearch for Randomforest\n",
    "rf_gs = GridSearchCV(rf_pipe, param_grid=rf_params, cv = 5, verbose = 1, n_jobs = -1)\n",
    "\n",
    "# Setting up GridSearch for CountVectorizer\n",
    "cv_gs = GridSearchCV(cv_pipe, param_grid=cvec_params, cv = 5, verbose = 1, n_jobs = -1)\n",
    "\n",
    "# Fitting CountVectorizer GS\n",
    "cv_gs.fit(X_train, y_train)\n",
    "\n",
    "# Setting up GridSearch for TFIDFVectorizer\n",
    "tvc_gs = GridSearchCV(tvc_pipe, param_grid=tf_params, cv = 5, verbose =1, n_jobs = -1)\n",
    "\n",
    "# Fitting CV GS\n",
    "tvc_gs.fit(X_train, y_train)\n",
    "\n",
    "# Fitting Randomforest CV GS\n",
    "rf_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8991991771361398"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9017188188629353"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9255014326647565"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvc_gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9206698986337594"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvc_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9747263242965248"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring Random Forest train\n",
    "rf_gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9237549581313353"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring Random Forest test\n",
    "rf_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a new df for  feature importance Random Forest\n",
    "rf_feat = pd.DataFrame(rf_pipe.steps[1][1].feature_importances_, rf_pipe.steps[0][1].get_feature_names(), columns=['importance'])\n",
    "rf_feat.sort_values('importance', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Top 20 Words in Random Forest\n",
    "plt.figure(figsize=(20,10))\n",
    "temp = rf_feat.sort_values('importance', ascending = False).head(20)\n",
    "plt.barh(temp.index, temp['importance'])\n",
    "plt.title('Top 20 Words', fontsize=40)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xlabel('Amount of Information Gained', fontsize=30)\n",
    "plt.ylabel('Word', fontsize=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_wordcloud(rf_feat.sort_values('importance', ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Random Forest Feat Importance to csv\n",
    "# rf_feat.to_csv('./rf_feat_imp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Corpus on Random Forest selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a transformed TFIDFVectorizer with best params\n",
    "tfid = TfidfVectorizer(stop_words='english', max_features=5000, ngram_range=(1, 1))\n",
    "\n",
    "# Creating a RandomForest with best params\n",
    "rf_p = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "# Creating corpus and vectorizing training\n",
    "train_tfid_tf = tfid.fit_transform(X_train)\n",
    "\n",
    "# Creating corpus and vectorizing testing\n",
    "test_tfid_tf = tfid.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new params and pipeline for RandomForest\n",
    "\n",
    "rf_p_params = {\n",
    "    'max_depth':[None],\n",
    "    'max_leaf_nodes': [100],\n",
    "    'min_samples_split': [100]\n",
    "}\n",
    "\n",
    "# Creating a new pipeline\n",
    "gs_rf = GridSearchCV(rf_p, param_grid=rf_p_params, cv = 5, n_jobs = -1)\n",
    "\n",
    "# Fitting model\n",
    "gs_rf.fit(train_tfid_df, y_train)\n",
    "\n",
    "# Scoring Training data\n",
    "gs_rf.score(train_tfid_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring Testing data\n",
    "gs_rf.score(test_tfid_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importance = pd.DataFrame(gs_rf.best_estimator_.feature_importances_, train_tfid_df.columns, columns=[ 'importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Corpus RandomForest on Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X, y Variables\n",
    "X, y = df['title'], df['subreddit']\n",
    "\n",
    "# Setting up train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a TFIDFVectorizer with best params\n",
    "tfid = TfidfVectorizer(stop_words='english', max_features=2000, ngram_range=(1, 1))\n",
    "\n",
    "# Creating a RandomForest with best params\n",
    "rf_p = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Creating corpus and vectorizing training\n",
    "train_tfid_tf = tfid.fit_transform(X_train)\n",
    "\n",
    "# Creating corpus and vectorizing testing\n",
    "test_tfid_tf = tfid.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new params and pipeline for RandomForest\n",
    "\n",
    "rf_p_params = {\n",
    "    'max_depth':[None],\n",
    "    'max_leaf_nodes': [50, 100, 500],\n",
    "    'min_samples_split': [20, 50, 100]\n",
    "}\n",
    "\n",
    "# Creating a new pipeline\n",
    "gs_rf = GridSearchCV(rf_p, param_grid=rf_p_params, cv = 5, n_jobs = -1)\n",
    "\n",
    "# Fitting model\n",
    "gs_rf.fit(train_tfid_df, y_train)\n",
    "\n",
    "# Scoring Training data\n",
    "gs_rf.score(train_tfid_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model file in Pickle (gs_rf)\n",
    "pkl_vect_rft = \"vect_rft.pkl\"  \n",
    "with open(pkl_vect_rft, 'wb') as file:  \n",
    "    pickle.dump(gs_rf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing model\n",
    "gs_rf.score(test_tfid_tf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Corpus on Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X, y Variables\n",
    "X, y = df['text'], df['subreddit']\n",
    "\n",
    "# Setting up train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a TFIDFVectorizer with best params\n",
    "tfid = TfidfVectorizer(stop_words=None, max_features=2000, ngram_range=(1, 1))\n",
    "\n",
    "# Creating a RandomForest with best params\n",
    "rf_p = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "# Creating corpus and vectorizing training\n",
    "train_tfid_tf = tfid.fit_transform(X_train)\n",
    "\n",
    "# Creating corpus and vectorizing testing\n",
    "test_tfid_tf = tfid.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new params and pipeline for RandomForest\n",
    "\n",
    "rf_p_params = {\n",
    "    'max_depth':[None],\n",
    "    'max_leaf_nodes': [50, 100, 500],\n",
    "    'min_samples_split': [20, 50, 100]\n",
    "}\n",
    "\n",
    "# Creating a new pipeline\n",
    "gs_rf = GridSearchCV(rf_p, param_grid=rf_p_params, cv = 5, n_jobs = -1)\n",
    "\n",
    "# Fitting model\n",
    "gs_rf.fit(train_tfid_df, y_train)\n",
    "\n",
    "# Scoring Training data\n",
    "gs_rf.score(train_tfid_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rf.score(test_tfid_tf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X, y Variables\n",
    "X, y = df['text'], df['subreddit']\n",
    "\n",
    "# Setting up train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a TFIDFVectorizer with best params\n",
    "tfid = TfidfVectorizer(stop_words='english', max_features=2000, ngram_range=(1, 1))\n",
    "\n",
    "# Creating corpus and vectorizing training\n",
    "train_tfid_tf = tfid.fit_transform(X_train)\n",
    "\n",
    "# Creating corpus and vectorizing testing\n",
    "test_tfid_tf = tfid.fit_transform(X_test)\n",
    "\n",
    "# Creating a RandomForest with best params\n",
    "rf_p = RandomForestClassifier(n_estimators=20)\n",
    "\n",
    "rf_p.fit(train_tfid_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a class for predictions random forest\n",
    "y_pred = rf_p.predict(test_tfid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Checking accuracy score of model\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a feature importance for RandomForest\n",
    "feat_importance = pd.DataFrame(rf_p.feature_importances_, train_tfid_df.columns, columns=['importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feature importance df sorted by importance\n",
    "feat_importance.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['text'], df['subreddit']\n",
    "\n",
    "# Setting up train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a TFIDFVectorizer with best params\n",
    "tfid = TfidfVectorizer(stop_words='english', max_features=2000, ngram_range=(1, 1))\n",
    "\n",
    "# Creating corpus and vectorizing training\n",
    "train_tfid_tf = tfid.fit_transform(X_train)\n",
    "\n",
    "# Creating corpus and vectorizing testing\n",
    "test_tfid_tf = tfid.fit_transform(X_test)\n",
    "\n",
    "# Creating a RandomForest with best params\n",
    "mb = MultinomialNB()\n",
    "\n",
    "mb.fit(train_tfid_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a class for predictions Multinomial NB\n",
    "y_pred = mb.predict(test_tfid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word = tfid.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times each token appears across all HAM messages\n",
    "word_count = mb.feature_count_[0, :]\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times each token appears across all SPAM messages\n",
    "spam_count = mb.feature_count_[1, :]\n",
    "spam_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a new df for word count in Multinomial Naives Bayes\n",
    "mb_df = pd.DataFrame({'word':train_word, 'count':word_count, 'spam':spam_count}).set_index('word')\n",
    "mb_df.sort_values('count', ascending = False)\n",
    "\n",
    "# Got help with this from RichieNG website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting for Multinomial Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting a hbar graph of Top 20 Words in MB\n",
    "plt.figure(figsize=(20,10))\n",
    "temp = mb_df.sort_values('count', ascending=False).head(20)\n",
    "plt.barh(temp.index, temp['count'])\n",
    "plt.title('Top 20 Words', fontsize=40)\n",
    "plt.xlabel('Count', fontsize=30)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.ylabel('Words', fontsize=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a word cloud of the Top 20 words in MB\n",
    "create_wordcloud(mb_df.sort_values('count', ascending = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Top 20 Words in Random Forest\n",
    "plt.figure(figsize=(20,10))\n",
    "temp = feat_importance.sort_values('importance', ascending=False).head(20)\n",
    "plt.barh(temp.index, temp['importance'])\n",
    "plt.title('Top 20 Words', fontsize=40)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xlabel('Frequency', fontsize=30)\n",
    "plt.ylabel('Word', fontsize=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud of Top 20 words in RandomForest\n",
    "create_wordcloud(feat_importance.sort_values('importance', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (dsi)",
   "language": "python",
   "name": "pycharm-32585468"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
